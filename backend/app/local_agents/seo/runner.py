"""
SEO Optimization Runner

Orchestrates SEO analysis and optimization using deterministic analysis 
combined with AI-powered optimization suggestions.
"""

import logging
from typing import Dict, List, Any, Optional
from agents import Runner

from .agent import seo_optimization_agent
from .prompts import SEO_ANALYSIS_PROMPT_TEMPLATE
from .helper_methods import (
    calculate_keyword_coverage,
    analyze_root_coverage,
    analyze_content_piece,
    prepare_keyword_data_for_analysis,
    format_keywords_for_prompt,
    calculate_character_usage,
    extract_keywords_from_content
)
from .schemas import (
    SEOAnalysisResult, CurrentSEO, OptimizedSEO, SEOComparison,
    KeywordCoverage, RootCoverage, ContentAnalysis
)

logger = logging.getLogger(__name__)


class SEORunner:
    """
    Runner for SEO optimization analysis and suggestions.
    
    Combines deterministic analysis of current SEO state with 
    AI-powered optimization recommendations.
    """
    
    def run_seo_analysis(
        self,
        scraped_product: Dict[str, Any],
        keyword_items: List[Dict[str, Any]],
        broad_search_volume_by_root: Optional[Dict[str, int]] = None,
        competitor_data: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        Run complete SEO analysis and optimization.
        
        Args:
            scraped_product: Product data from research agent
            keyword_items: Categorized and scored keywords from scoring agent
            broad_search_volume_by_root: Root volume data (optional)
            competitor_data: Competitor ASIN data for Task 6 analysis (optional)
            
        Returns:
            Complete SEO analysis result
        """
        
        try:
            logger.info("ðŸ” Starting SEO analysis and optimization")
            
            # Step 1: Extract current listing content
            current_content = self._extract_current_content(scraped_product)
            logger.info(f"ðŸ“„ Extracted current content: {len(current_content.get('bullets', []))} bullets")
            
            # Step 2: Prepare keyword data for analysis
            keyword_data = prepare_keyword_data_for_analysis(keyword_items)
            logger.info(f"ðŸ“Š Prepared keyword data: {keyword_data['total_keywords']} total keywords")
            
            # Step 3: Perform deterministic current SEO analysis
            current_seo = self._analyze_current_seo(current_content, keyword_data, broad_search_volume_by_root)
            logger.info(f"âœ… Current SEO analysis complete: {current_seo.keyword_coverage.coverage_percentage}% coverage")
            
            # Step 4: Task 6 - Analyze competitor titles for benefit-focused optimization
            competitor_analysis = None
            if competitor_data and len(competitor_data) > 0:
                try:
                    from .subagents.competitor_title_analysis_agent import apply_competitor_title_optimization_ai
                    competitor_analysis = apply_competitor_title_optimization_ai(
                        current_content=current_content,
                        competitor_data=competitor_data,
                        keyword_data=keyword_data,
                        product_context=scraped_product
                    )
                    logger.info(f"ðŸ† Task 6: Analyzed {len(competitor_data)} competitors for benefit optimization")
                except Exception as e:
                    logger.warning(f"ðŸŸ¡ Task 6: Competitor analysis failed: {e}")
                    competitor_analysis = None
            else:
                logger.info("ðŸ“ Task 6: No competitor data provided, skipping competitor analysis")
            
            # Step 5: Generate AI-powered optimization suggestions (enhanced with competitor insights)
            optimized_seo = self._generate_ai_optimizations(
                current_content, keyword_data, scraped_product, competitor_analysis
            )
            logger.info("ðŸ¤– AI optimization suggestions generated")
            
            # Step 5: Calculate comparison metrics
            comparison = self._calculate_comparison_metrics(current_seo, optimized_seo, keyword_data)
            logger.info("ðŸ“ˆ Comparison metrics calculated")
            
            # Step 6: Assemble final result
            result = SEOAnalysisResult(
                current_seo=current_seo,
                optimized_seo=optimized_seo,
                comparison=comparison,
                product_context={
                    "title": scraped_product.get("title", ""),
                    "brand": current_content.get("brand", ""),
                    "category": "Unknown"  # Could be enhanced with category detection
                },
                analysis_metadata={
                    "total_keywords_analyzed": len(keyword_items),
                    "relevant_keywords_count": len(keyword_data["relevant_keywords"]),
                    "high_intent_keywords_count": len(keyword_data["high_intent_keywords"]),
                    "optimization_method": "ai" if self._should_use_ai_optimization() else "rule_based"
                }
            )
            
            logger.info("âœ… SEO analysis complete")
            return {
                "success": True,
                "analysis": result.model_dump(),
                "summary": {
                    "current_coverage": f"{current_seo.keyword_coverage.coverage_percentage}%",
                    "optimization_opportunities": len(current_seo.keyword_coverage.missing_high_intent),
                    "method": "ai" if self._should_use_ai_optimization() else "rule_based"
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ SEO analysis failed: {str(e)}", exc_info=True)
            return {
                "success": False,
                "error": f"SEO analysis failed: {str(e)}"
            }
    
    def _extract_current_content(self, scraped_product: Dict[str, Any]) -> Dict[str, Any]:
        """Extract current listing content from scraped product data."""
        
        # Extract title
        title = scraped_product.get("title", "")
        
        # Extract bullets from elements
        bullets = []
        elements = scraped_product.get("elements", {})
        feature_bullets = elements.get("feature-bullets", {})
        if feature_bullets.get("present"):
            bullets = feature_bullets.get("bullets", [])
        
        # Extract backend keywords (if available - may not be in scraped data)
        backend_keywords = []
        
        # Extract brand if available
        brand = ""
        overview = elements.get("productOverview_feature_div", {})
        if overview.get("present"):
            kv_data = overview.get("kv", {})
            brand = kv_data.get("Brand", "")
        
        return {
            "title": title,
            "bullets": bullets,
            "backend_keywords": backend_keywords,
            "brand": brand
        }
    
    def _analyze_current_seo(
        self, 
        current_content: Dict[str, Any], 
        keyword_data: Dict[str, Any],
        root_volumes: Optional[Dict[str, int]] = None
    ) -> CurrentSEO:
        """Perform deterministic analysis of current SEO state."""
        
        # Get relevant keywords for analysis
        all_relevant = keyword_data["relevant_keywords"] + keyword_data["design_keywords"]
        keyword_phrases = [kw.get("phrase", "") for kw in all_relevant]
        
        # Analyze keyword coverage
        coverage_data = calculate_keyword_coverage(current_content, all_relevant)
        keyword_coverage = KeywordCoverage(**coverage_data)
        
        # Analyze root coverage
        if root_volumes:
            root_data = analyze_root_coverage(current_content, root_volumes)
            root_coverage = RootCoverage(**root_data)
        else:
            # Use aggregated root volumes from keyword data
            root_coverage = RootCoverage(
                total_roots=len(keyword_data["root_volumes"]),
                covered_roots=0,
                coverage_percentage=0.0,
                missing_roots=list(keyword_data["root_volumes"].keys()),
                root_volumes=keyword_data["root_volumes"]
            )
        
        # Analyze individual content pieces
        title_analysis = ContentAnalysis(**analyze_content_piece(
            current_content["title"], keyword_phrases
        ))
        
        bullets_analysis = []
        for bullet in current_content["bullets"]:
            analysis = ContentAnalysis(**analyze_content_piece(bullet, keyword_phrases))
            bullets_analysis.append(analysis)
        
        # Calculate character usage
        char_usage = calculate_character_usage(current_content)
        
        return CurrentSEO(
            title_analysis=title_analysis,
            bullets_analysis=bullets_analysis,
            backend_keywords=current_content["backend_keywords"],
            keyword_coverage=keyword_coverage,
            root_coverage=root_coverage,
            total_character_usage=char_usage
        )
    
    def _generate_ai_optimizations(
        self,
        current_content: Dict[str, Any],
        keyword_data: Dict[str, Any], 
        scraped_product: Dict[str, Any],
        competitor_analysis: Optional[Dict[str, Any]] = None
    ) -> OptimizedSEO:
        """Generate AI-powered optimization suggestions with Task 7 Amazon compliance."""
        
        # Task 7: Use AI-powered Amazon Compliance Agent for guidelines compliance and 80-char optimization
        try:
            from app.local_agents.seo.subagents.amazon_compliance_agent import apply_amazon_compliance_ai
            
            # Extract product context for AI agent
            product_context = {
                "brand": scraped_product.get("elements", {}).get("brand", {}).get("text", ""),
                "category": scraped_product.get("category", ""),
                "title": current_content.get("title", "")
            }
            
            # Apply AI-powered Amazon compliance optimization (Task 7) with Task 6 competitor insights
            compliance_result = apply_amazon_compliance_ai(
                current_content=current_content,
                keyword_data=keyword_data,
                product_context=product_context,
                competitor_analysis=competitor_analysis
            )
            
            logger.info("[Task7-AI] Applied Amazon Guidelines Compliance with 80-character optimization")
            
            # Convert compliance result to OptimizedSEO format
            return self._convert_compliance_result_to_optimized_seo(compliance_result, keyword_data)
            
        except Exception as compliance_error:
            logger.warning(f"[Task7-AI] Amazon compliance agent failed, falling back to standard AI: {compliance_error}")
            
            # Fallback to original AI optimization if Task 7 agent fails
            prompt_data = {
                "current_title": current_content["title"],
                "current_bullets": "\n".join([f"â€¢ {bullet}" for bullet in current_content["bullets"]]),
                "backend_keywords": " ".join(current_content["backend_keywords"]) if current_content["backend_keywords"] else "None",
                "total_keywords": keyword_data["total_keywords"],
                "relevant_keywords": format_keywords_for_prompt(keyword_data["relevant_keywords"]),
                "design_keywords": format_keywords_for_prompt(keyword_data["design_keywords"]),
                "root_volumes": str(keyword_data["root_volumes"]),
                "high_intent_keywords": format_keywords_for_prompt(keyword_data["high_intent_keywords"]),
                "high_volume_keywords": format_keywords_for_prompt(keyword_data["high_volume_keywords"]),
                "product_context": str(scraped_product.get("elements", {}).get("productOverview_feature_div", {}))
            }
            
            # Enhanced prompt with Task 7 requirements
            enhanced_prompt = SEO_ANALYSIS_PROMPT_TEMPLATE.format(**prompt_data)
            enhanced_prompt += """

## REQUIREMENTS:
- Follow Amazon Title Guidelines (https://sellercentral.amazon.com/help/hub/reference/external/GYTR6SYGFA5E3EQC?locale=en-US)
- Follow Amazon Bullet Point Guidelines (https://sellercentral.amazon.com/help/hub/reference/external/GX5L8BF8GLMML6CX?locale=en-US)
- OPTIMIZE FIRST 80 CHARACTERS: Must include main keyword root + design-specific keyword root + key benefit
- No promotional language, proper capitalization, compliance with all Amazon guidelines
"""
            
            try:
                # Run standard AI agent with enhanced prompt
                result = Runner.run_sync(seo_optimization_agent, enhanced_prompt)
                ai_output = result.final_output
                return self._parse_ai_output_to_optimized_seo(ai_output, keyword_data)

            except Exception as e:
                logger.error(f"Both Task and fallback AI optimization failed: {e}")
                raise
    
    def _convert_compliance_result_to_optimized_seo(
        self, 
        compliance_result: Dict[str, Any], 
        keyword_data: Dict[str, Any]
    ) -> OptimizedSEO:
        """Convert compliance result to OptimizedSEO format."""
        from .schemas import OptimizedContent
        
        # Extract optimized title
        title_data = compliance_result.get("optimized_title", {})
        optimized_title = OptimizedContent(
            content=title_data.get("content", ""),
            keywords_included=title_data.get("keywords_included", []),
            improvements=[
                f"Amazon Guidelines Compliant",
                f"80-char optimized: {title_data.get('first_80_chars', '')[:50]}...",
                f"Main root included: {title_data.get('main_root_included', False)}",
                f"Design root included: {title_data.get('design_root_included', False)}"
            ],
            character_count=title_data.get("character_count", 0)
        )
        
        # Extract optimized bullets
        bullets_data = compliance_result.get("optimized_bullets", [])
        optimized_bullets = []
        for bullet in bullets_data:
            optimized_bullets.append(OptimizedContent(
                content=bullet.get("content", ""),
                keywords_included=bullet.get("keywords_included", []),
                improvements=[
                    f"Benefit-focused: {bullet.get('primary_benefit', '')}",
                    f"Guideline compliant: {bullet.get('guideline_compliance', 'PASS')}"
                ],
                character_count=bullet.get("character_count", 0)
            ))
        
        # Generate backend keywords (enhanced with compliance awareness)
        relevant_keywords = keyword_data.get("relevant_keywords", [])
        design_keywords = keyword_data.get("design_keywords", [])
        all_keywords = relevant_keywords + design_keywords
        
        # Extract backend keywords not used in title/bullets
        title_content = title_data.get("content", "").lower()
        bullet_content = " ".join([b.get("content", "") for b in bullets_data]).lower()
        
        backend_keywords = []
        for kw in all_keywords[:15]:  # Top 15
            phrase = kw.get("phrase", "")
            if phrase.lower() not in title_content and phrase.lower() not in bullet_content:
                backend_keywords.append(phrase)
        
        # Build strategy explanation
        strategy = compliance_result.get("strategy", {})
        
        return OptimizedSEO(
            optimized_title=optimized_title,
            optimized_bullets=optimized_bullets,
            optimized_backend_keywords=backend_keywords,
            keyword_strategy={
                "amazon_compliance": "ENFORCED",
                "first_80_optimization": strategy.get("first_80_optimization", "Applied"),
                "keyword_integration": strategy.get("keyword_integration", "Optimized"),
                "guideline_compliance": "STRICT"
            },
            rationale=(
                f"Task 7 Amazon Guidelines Compliance Applied: "
                f"{strategy.get('compliance_approach', 'AI-powered optimization with strict guideline adherence')}. "
                f"First 80 characters optimized for mobile viewing with main and design-specific keyword roots. "
                f"All content verified for Amazon policy compliance."
            )
        )
    
    def _generate_rule_based_optimizations(
        self,
        current_content: Dict[str, Any],
        keyword_data: Dict[str, Any]
    ) -> OptimizedSEO:
        """Generate rule-based optimization suggestions that fulfill MVP requirements."""
        
        # Get keywords by category and intent
        relevant_keywords = keyword_data["relevant_keywords"]
        design_keywords = keyword_data["design_keywords"]
        high_intent_keywords = keyword_data["high_intent_keywords"]
        high_volume_keywords = keyword_data["high_volume_keywords"]
        
        # Combine relevant + design for optimization
        all_good_keywords = relevant_keywords + design_keywords
        
        # Sort by search volume (descending) and intent score (descending)
        sorted_keywords = sorted(all_good_keywords, 
                               key=lambda x: (x.get("intent_score", 0), x.get("search_volume", 0)), 
                               reverse=True)
        
        # Get all unique roots from good keywords
        all_roots = set()
        root_to_best_keyword = {}
        for kw in sorted_keywords:
            root = kw.get("root", "")
            if root and root not in root_to_best_keyword:
                root_to_best_keyword[root] = kw["phrase"]
                all_roots.add(root)
        
        # Select top keywords for title optimization
        # Prioritize: high intent + high volume + covers different roots
        title_keywords = []
        used_roots = set()
        
        # First, get highest intent keywords that cover different roots
        for kw in sorted_keywords[:15]:  # Look at top 15
            root = kw.get("root", "")
            phrase = kw.get("phrase", "")
            intent = kw.get("intent_score", 0)
            volume = kw.get("search_volume", 0)
            
            # Prioritize high intent (2+) and good volume (300+)
            if intent >= 2 and volume >= 300 and root not in used_roots and len(title_keywords) < 4:
                title_keywords.append(phrase)
                used_roots.add(root)
        
        # Fill remaining slots with high-volume terms
        for kw in sorted_keywords:
            phrase = kw.get("phrase", "")
            volume = kw.get("search_volume", 0)
            if len(title_keywords) < 5 and phrase not in title_keywords and volume >= 400:
                title_keywords.append(phrase)
        
        # Create natural-sounding optimized title
        brand = current_content.get("brand", "BREWER")
        primary_keyword = title_keywords[0] if title_keywords else "freeze dried strawberries"
        
        # Build title with natural flow
        title_parts = []
        if brand:
            title_parts.append(brand)
        
        # Add pack info
        title_parts.append("Bulk")
        
        # Add primary keyword
        title_parts.append(primary_keyword.title())
        
        # Add key attributes
        title_parts.append("- Organic")
        title_parts.append("No Sugar Added")
        
        # Add secondary keyword if different
        if len(title_keywords) > 1:
            secondary = title_keywords[1]
            if "organic" not in secondary.lower():
                title_parts.append(f"Premium {secondary.title()}")
        
        # Add use cases
        title_parts.append("for Snacking, Baking & Travel")
        
        optimized_title_content = " ".join(title_parts)[:200]  # Amazon limit
        
        optimized_title = {
            "content": optimized_title_content,
            "keywords_included": title_keywords[:3],
            "improvements": [
                f"Covers {len(used_roots)} keyword roots",
                f"Uses {len([k for k in all_good_keywords if k.get('intent_score', 0) >= 2])} high-intent keywords",
                f"Replaces low-volume terms with {sum(k.get('search_volume', 0) for k in all_good_keywords[:3])//1000}k+ volume terms",
                "Natural, readable format"
            ],
            "character_count": len(optimized_title_content)
        }
        
        # Create optimized bullets focusing on different keyword clusters
        optimized_bullets = []
        bullet_templates = [
            ("Flavor & Quality", "Enjoy the naturally sweet, tangy crunch of {} bursting with real fruit flavor."),
            ("Organic & Healthy", "Our {} are organic, no sugar added, and made from farm-fresh strawberries."),
            ("Versatile Use", "Perfect {} for baking, smoothies, cereals, and on-the-go snacking."),
            ("Travel Ready", "Lightweight and shelf-stable {} ideal for hiking, camping, and travel adventures."),
            ("Bulk Value", "Buy in bulk and enjoy premium quality {} whenever you need them.")
        ]
        
        for i, (focus, template) in enumerate(bullet_templates):
            if i < len(sorted_keywords):
                keyword = sorted_keywords[i]["phrase"]
                bullet_content = f"âœ… {focus}: " + template.format(keyword)
                optimized_bullets.append({
                    "content": bullet_content,
                    "keywords_included": [keyword],
                    "improvements": [f"Focused on {focus.lower()}", "Clear value proposition"],
                    "character_count": len(bullet_content)
                })
        
        # Backend keywords: Use remaining good keywords not in title/bullets
        used_in_content = set(title_keywords)
        for bullet in optimized_bullets:
            used_in_content.update(bullet["keywords_included"])
        
        backend_keywords = []
        for kw in sorted_keywords:
            phrase = kw["phrase"]
            if phrase not in used_in_content and len(backend_keywords) < 15:
                backend_keywords.append(phrase)
        
        # Task 11: Apply AI-powered variant optimization before generating variations
        try:
            from app.local_agents.scoring.subagents.keyword_variant_agent import apply_variant_optimization_ai
            # Use AI to remove redundant singular/plural variants from good keywords
            optimized_keywords = apply_variant_optimization_ai(all_good_keywords)
            logger.info(f"[Task11-AI] Applied AI variant optimization: {len(all_good_keywords)} -> {len(optimized_keywords)} keywords")
        except Exception as e:
            logger.warning(f"[Task11-AI] AI variant optimization failed, using original keywords: {e}")
            optimized_keywords = all_good_keywords
        
        # Add misspellings and variations (using optimized keyword list)
        variations = []
        for kw in optimized_keywords:
            phrase = kw["phrase"]
            # Add common variations (punctuation, spacing only - no singular/plural thanks to AI)
            if "freeze dried" in phrase:
                variations.append(phrase.replace("freeze dried", "freeze-dried"))
                variations.append(phrase.replace("freeze dried", "freezedried"))
        
        # Remove duplicates and already used variations
        filtered_variations = []
        for variation in variations:
            if variation not in used_in_content and variation not in backend_keywords:
                filtered_variations.append(variation)
        
        backend_keywords.extend(filtered_variations[:5])
        
        from .schemas import OptimizedContent
        
        return OptimizedSEO(
            optimized_title=OptimizedContent(**optimized_title),
            optimized_bullets=[OptimizedContent(**bullet) for bullet in optimized_bullets],
            optimized_backend_keywords=backend_keywords,
            keyword_strategy={
                "primary_focus": f"Cover {len(all_roots)} keyword roots",
                "secondary_focus": f"Prioritize {len(high_intent_keywords)} high-intent keywords",
                "volume_strategy": f"Replace low-volume with {sum(k.get('search_volume', 0) for k in high_volume_keywords[:5])//1000}k+ volume terms",
                "character_optimization": True,
                "root_coverage": f"{len(used_roots)}/{len(all_roots)} roots covered"
            },
            rationale=f"Enhanced optimization covering {len(all_roots)} keyword roots, prioritizing {len(high_intent_keywords)} high-intent terms, and replacing lower-volume keywords with better search volume options. Achieved natural readability while maximizing SEO potential."
        )
    
    def _parse_ai_output_to_optimized_seo(self, ai_output: Any, keyword_data: Dict[str, Any]) -> OptimizedSEO:
        """Parse AI output into OptimizedSEO structure."""
        from .schemas import OptimizedContent
        data = ai_output

        # If output is JSON string, parse it
        if isinstance(data, str):
            import json
            data = json.loads(data)

        # If output is a Pydantic model, convert to dict
        try:
            if hasattr(data, 'model_dump'):
                data = data.model_dump()
        except Exception:
            pass

        # If the agent returned a full SEOAnalysisResult, extract optimized_seo
        if isinstance(data, dict) and 'optimized_seo' in data:
            data = data['optimized_seo']

        # At this point, data should be the optimized_seo dict
        if not isinstance(data, dict):
            raise ValueError("AI output is not in expected optimized_seo dict format")

        # Build OptimizedSEO from dict tolerantly
        optimized_title = data.get('optimized_title') or {}
        optimized_bullets = data.get('optimized_bullets') or []
        optimized_backend_keywords = data.get('optimized_backend_keywords') or []
        keyword_strategy = data.get('keyword_strategy') or {}
        rationale = data.get('rationale') or ""

        # Normalize bullets and title into OptimizedContent
        def to_opt_content(obj: dict) -> OptimizedContent:
            return OptimizedContent(
                content=obj.get('content', ''),
                keywords_included=list(obj.get('keywords_included', [])),
                improvements=list(obj.get('improvements', [])),
                character_count=int(obj.get('character_count', len(obj.get('content', ''))))
            )

        title_content = to_opt_content(optimized_title)
        bullets_content = [to_opt_content(b) for b in optimized_bullets if isinstance(b, dict)]

        return OptimizedSEO(
            optimized_title=title_content,
            optimized_bullets=bullets_content,
            optimized_backend_keywords=list(optimized_backend_keywords),
            keyword_strategy=keyword_strategy,
            rationale=rationale
        )
    
    def _calculate_comparison_metrics(
        self, 
        current_seo: CurrentSEO, 
        optimized_seo: OptimizedSEO,
        keyword_data: Dict[str, Any]
    ) -> SEOComparison:
        """Calculate before/after comparison metrics from actual content."""

        # Build phrase lists and maps
        all_keywords = keyword_data.get("relevant_keywords", []) + keyword_data.get("design_keywords", [])
        phrases = [kw.get("phrase", "") for kw in all_keywords if kw.get("phrase")]
        phrase_to_volume = {kw.get("phrase", ""): kw.get("search_volume", 0) for kw in all_keywords}
        high_intent_phrases = [kw.get("phrase", "") for kw in keyword_data.get("high_intent_keywords", []) if kw.get("phrase")]

        # Current content strings
        current_title = current_seo.title_analysis.content
        current_bullets = [b.content for b in current_seo.bullets_analysis]
        current_backend = current_seo.backend_keywords
        current_text = " ".join([current_title] + current_bullets + [" ".join(current_backend)])

        # Optimized content strings
        opt_title = optimized_seo.optimized_title.content
        opt_bullets = [b.content for b in optimized_seo.optimized_bullets]
        opt_backend = optimized_seo.optimized_backend_keywords
        opt_text = " ".join([opt_title] + opt_bullets + [" ".join(opt_backend)])

        # Coverage details
        current_found, _ = extract_keywords_from_content(current_text, phrases)
        opt_found, _ = extract_keywords_from_content(opt_text, phrases)

        total_kw = len(phrases)
        before_cov = len(set(current_found))
        after_cov = len(set(opt_found))
        before_pct = round((before_cov / total_kw * 100), 2) if total_kw else 0.0
        after_pct = round((after_cov / total_kw * 100), 2) if total_kw else 0.0
        new_added = [p for p in set(opt_found) if p not in set(current_found)]

        coverage_improvement = {
            "total_keywords": total_kw,
            "before_covered": before_cov,
            "after_covered": after_cov,
            "before_coverage_pct": before_pct,
            "after_coverage_pct": after_pct,
            "delta_pct_points": round(after_pct - before_pct, 2),
            "new_keywords_added": sorted(new_added)[:20]
        }

        # Intent coverage
        before_hi_intent = len([p for p in high_intent_phrases if p in set(current_found)])
        after_hi_intent = len([p for p in high_intent_phrases if p in set(opt_found)])
        intent_improvement = {
            "high_intent_total": len(high_intent_phrases),
            "before_covered": before_hi_intent,
            "after_covered": after_hi_intent,
            "delta": max(0, after_hi_intent - before_hi_intent)
        }

        # Volume capture (approximate)
        before_vol = sum(phrase_to_volume.get(p, 0) for p in set(current_found))
        after_vol = sum(phrase_to_volume.get(p, 0) for p in set(opt_found))
        volume_improvement = {
            "estimated_volume_before": before_vol,
            "estimated_volume_after": after_vol,
            "delta_volume": max(0, after_vol - before_vol)
        }

        # Character efficiency
        title_before = len(current_title or "")
        title_after = len(opt_title or "")
        backend_before = len(" ".join(current_backend) if current_backend else "")
        backend_after = len(" ".join(opt_backend) if opt_backend else "")
        char_efficiency = {
            "title_limit": 200,
            "title_before": title_before,
            "title_after": title_after,
            "title_utilization_before_pct": round((title_before / 200) * 100, 1) if 200 else 0,
            "title_utilization_after_pct": round((title_after / 200) * 100, 1) if 200 else 0,
            "backend_limit": 249,
            "backend_before": backend_before,
            "backend_after": backend_after,
        }

        # Summary metrics (simple heuristic)
        summary_metrics = {
            "overall_improvement_score": round(min(10.0, (after_pct - before_pct) / 10 + (after_hi_intent - before_hi_intent) / 2), 2),
            "priority_recommendations": max(1, min(5, len(new_added) // 4)),
        }

        return SEOComparison(
            coverage_improvement=coverage_improvement,
            intent_improvement=intent_improvement,
            volume_improvement=volume_improvement,
            character_efficiency=char_efficiency,
            summary_metrics=summary_metrics
        )
    
    def _should_use_ai_optimization(self) -> bool:
        """Always use AI optimization (no rule-based fallback)."""
        return True