# HOTFIX: Stats Structure Validation Error

## üêõ Problem

After implementing Task 5 (switching to `gpt-4o-mini`), the keyword categorization agent started returning an **incorrect JSON structure** for the `stats` field, causing Pydantic validation errors:

### Error Message:

```
pydantic_core._pydantic_core.ValidationError: 2 validation errors for KeywordAnalysisResult
stats.count.[key]
  Input should be 'Relevant', 'Design-Specific', 'Irrelevant', 'Branded', 'Spanish' or 'Outlier'
stats.count.count
  Field required
```

### What the AI Was Returning (WRONG):

```json
{
  "stats": {
    "count": {
      "Branded": 1,
      "Spanish": 0,
      "Design-Specific": 0,
      "Irrelevant": 85,
      "Relevant": 24,
      "Outlier": 0
    }
  }
}
```

### What the Schema Expects (CORRECT):

```json
{
  "stats": {
    "Relevant": { "count": 24, "examples": ["keyword1", "keyword2"] },
    "Design-Specific": { "count": 0, "examples": [] },
    "Irrelevant": { "count": 85, "examples": ["bad keyword"] },
    "Branded": { "count": 1, "examples": ["brand name"] },
    "Spanish": { "count": 0, "examples": [] },
    "Outlier": { "count": 0, "examples": [] }
  }
}
```

---

## üîç Root Cause

When we switched from `gpt-5-2025-08-07` to `gpt-4o-mini` for speed optimization (Task 5):

1. The **prompt didn't explicitly show the `stats` field structure**
2. `gpt-4o-mini` inferred a different (incorrect) structure than GPT-5
3. The schema (`KeywordAnalysisResult`) expects: `Dict[KeywordCategory, CategoryStats]`
4. But AI was returning: `Dict[str, Dict[str, int]]` (nested wrong way)

---

## ‚úÖ Solution

Updated `backend/app/local_agents/keyword/prompts.py` to:

1. **Show complete JSON example** including the `stats` field
2. **Add explicit section** explaining the stats structure with examples
3. **Show WRONG vs CORRECT** format to prevent confusion

### Changes Made:

```python
# Added to the Output Format section:
"stats": {
  "Relevant": { "count": 0, "examples": [] },
  "Design-Specific": { "count": 0, "examples": [] },
  "Irrelevant": { "count": 0, "examples": [] },
  "Branded": { "count": 0, "examples": [] },
  "Spanish": { "count": 0, "examples": [] },
  "Outlier": { "count": 0, "examples": [] }
}

**CRITICAL - Stats Structure**:
- The `stats` field MUST be a dictionary where keys are category names
- Each category MUST have an object with "count" (integer) and "examples" (array)
- WRONG: `"stats": { "count": { "Relevant": 5 } }`
- CORRECT: `"stats": { "Relevant": { "count": 5, "examples": [...] } }`
```

---

## üìã Schema Reference

From `backend/app/local_agents/keyword/schemas.py`:

```python
class CategoryStats(BaseModel):
    count: int = Field(..., description="Number of keywords in this category")
    examples: List[str] = Field(default_factory=list, description="Up to a few example phrases")

class KeywordAnalysisResult(BaseModel):
    product_context: Dict
    items: List[KeywordData]
    stats: Dict[KeywordCategory, CategoryStats]  # This is what was failing!
```

The `stats` field expects a dictionary where:

- **Key**: `KeywordCategory` enum value (e.g., "Relevant", "Branded")
- **Value**: `CategoryStats` object with `count` and `examples` fields

---

## üß™ Testing

To verify the fix works:

```bash
cd backend
# Restart your backend server to reload the updated prompt
# Then test with a real API call
```

The error should no longer occur, and the AI should return the correct structure.

---

## üìù Files Modified

1. `backend/app/local_agents/keyword/prompts.py`
   - Updated `# Output Format` section
   - Added explicit `stats` field structure with example
   - Added **CRITICAL - Stats Structure** warning section

---

## ‚úÖ Resolution

**Status**: ‚úÖ **FIXED**

The prompt now explicitly shows the correct JSON structure for the `stats` field, preventing `gpt-4o-mini` from inferring an incorrect format.

---

## üí° Lesson Learned

When switching AI models (especially to smaller/faster models like `gpt-4o-mini`):

- **Always provide explicit output examples** in prompts
- **Smaller models need more guidance** than larger models
- **Test the full pipeline** after model changes to catch format issues early

